{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot,image\n",
    "\n",
    "shanthiPic = '/src/shanthiPictures/101ND750/DSC_0181.JPG'\n",
    "img = image.imread(shanthiPic)\n",
    "imgplot=pyplot.imshow(img)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def calculateMD5Hash(file):\n",
    "    fh = open(file, \"rb\")\n",
    "    return hashlib.md5(fh.read()).hexdigest()\n",
    "    \n",
    "\n",
    "shanthiPic = '/src/data/testPythonDuplicateFinder/2015-09/IMG_1313.JPG'\n",
    "print(calculateMD5Hash(shanthiPic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### List Extention ###\n",
    "from os import walk as oswalk\n",
    "from os.path import join as osjoin\n",
    "from time import time\n",
    "inputPath = '/src/data/testPythonDuplicateFinder'\n",
    "startTime = time()\n",
    "imageHashes = []\n",
    "images =[]\n",
    "for root, dirs, files in oswalk(inputPath):\n",
    "    images = [osjoin(root, f) for f in files]\n",
    "    imageHashes.extend([(image, calculateMD5Hash(image)) for image in images])\n",
    "\n",
    "print('Took {}s for {} images'.format(time() - startTime, len(imageHashes)))\n",
    "print ('images {}'.format(len(images)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from os import walk as oswalk\n",
    "from os.path import join as osjoin\n",
    "from time import time\n",
    "import hashlib\n",
    "def calculateMD5Hash(file):\n",
    "    fh = open(file, \"rb\")\n",
    "    return hashlib.md5(fh.read()).hexdigest()\n",
    "    \n",
    "inputPaths = ['/src/data/testPythonDuplicateFinder']\n",
    "duplicatesPath = '/src/data/duplicates'\n",
    "inputImages = []\n",
    "for inputPath in inputPaths:\n",
    "    for root, dirs, files in oswalk(inputPath):\n",
    "        inputImages.extend(img for img in [osjoin(root, f) for f in files] if img not in inputImages)\n",
    "\n",
    "\n",
    "startTime = time()\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_file = {executor.submit(calculateMD5Hash, inputFile): inputFile for inputFile in inputImages}\n",
    "    imageHashMap = {}\n",
    "    for future in concurrent.futures.as_completed(future_to_file):\n",
    "        file = future_to_file[future]\n",
    "        try:\n",
    "            md5Hash = future.result()\n",
    "            \n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (file, exc))\n",
    "        else:\n",
    "            if md5Hash in imageHashMap: \n",
    "                imageHashMap[md5Hash].extend([file])\n",
    "            else:\n",
    "                imageHashMap[md5Hash] = [file]\n",
    "\n",
    "print (imageHashMap)\n",
    "print('Took {}s for  {} images'.format(time() - startTime, len(inputImages)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "imageHashMapJson = json.JSONEncoder(sort_keys=True, indent=0).encode(imageHashMap)\n",
    "print (imageHashMapJson)\n",
    "with open('/src/data/fileHashes.json', 'w') as jsonFile:\n",
    "    jsonFile.write(imageHashMapJson)\n",
    "\n",
    "#json.dump(imageHashMap, '/src/data/fileHashes.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two ways to log an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:hello\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-77-8eb620650385>\", line 9, in <module>\n",
      "    print (os.path.normpath('/data'))\n",
      "NameError: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-77-8eb620650385>\", line 3, in <module>\n",
      "    print (os.path.normpath('/data'))\n",
      "NameError: name 'os' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Two ways to log exception\n",
    "import traceback\n",
    "try:\n",
    "    print (os.path.normpath('/data'))\n",
    "except :\n",
    "    print (traceback.format_exc())\n",
    "    \n",
    "import logging\n",
    "try:\n",
    "    print (os.path.normpath('/data'))\n",
    "except :\n",
    "    logging.exception(\"hello\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## Data Setup\n",
    "rm -rf /src/data/test1/ /src/data/test2/ /src/data/duplicates\n",
    "cp -r '/src/data/test1 (copy)/' '/src/data/test1/'\n",
    "cp -r '/src/data/test2 (copy)/' '/src/data/test2/'\n",
    "cp -r '/src/data/test2 (copy)/' '/src/data/test2/2015-10/'\n",
    "cp -r '/src/data/test1 (copy)/' '/src/data/test1/2015-09/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "#%%time\n",
    "## Start by loading the uniqueFiles list\n",
    "## Prepare the list of unique file paths\n",
    "## Find duplicates using the hash\n",
    "## move duplicates to duplicates folder\n",
    "## the run should output [hash, originalFilePath, [duplciate 1 path, /duplicate 2 path, duplicate 3,....]]\n",
    "## save this uniqueFiles list along with the hash [hash, original file path]\n",
    "import json\n",
    "from os import walk, path, sep, makedirs\n",
    "\n",
    "import concurrent.futures\n",
    "import hashlib\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "import threading\n",
    "makeDirLock = threading.Lock()\n",
    "\n",
    "## loads the file paths and hashes that are previously known\n",
    "## also ensures that all those files are actually present on the disk\n",
    "def loadUniqueFilesMap(uniqueFilesMapFile):\n",
    "    uniqueFilesMap = json.load(open(uniqueFilesMapFile)) if path.exists(uniqueFilesMapFile) else {}\n",
    "     # We can use a with statement to ensure threads are cleaned up promptly\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        future_to_file = {executor.submit(path.exists, inputFile): inputFile for inputFile in uniqueFilesMap.values()}\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            if (not future.result()):\n",
    "                raise OSError(\"File not found : {}\".format(future_to_file[future]))\n",
    "\n",
    "    return uniqueFilesMap\n",
    "\n",
    "## creates and returns a list of all the file paths from the inputPaths that are not\n",
    "## known(not present in uniqueFilesMap)\n",
    "def buildInputFilesList(inputPaths, uniqueFilesMap):\n",
    "    inputFiles = []\n",
    "    for inputPath in inputPaths:\n",
    "        for root, dirs, files in walk(inputPath):\n",
    "            inputFiles.extend(img for img in [path.join(root, f) for f in files] if img not in inputFiles if img not in uniqueFilesMap.values())\n",
    "    return inputFiles\n",
    "\n",
    "def calculateMD5Hash(file):\n",
    "    fh = open(file, \"rb\")\n",
    "    return hashlib.md5(fh.read()).hexdigest()\n",
    "\n",
    "## find all the inputFiles that are already present in uniqueFilesMap and add them to the duplicateFilesMap\n",
    "## all the new files frim teh inputFiles get added to uniqueFilesMap\n",
    "def checkForDuplicates(inputFiles, uniqueFilesMap):\n",
    "    duplicateFilesMap = {}\n",
    "    # We can use a with statement to ensure threads are cleaned up promptly\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        future_to_file = {executor.submit(calculateMD5Hash, inputFile): inputFile for inputFile in inputFiles}\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file = future_to_file[future]\n",
    "            md5Hash = future.result()\n",
    "            if md5Hash in uniqueFilesMap:\n",
    "                if md5Hash in duplicateFilesMap:\n",
    "                    duplicateFilesMap[md5Hash].append(file)\n",
    "                else:\n",
    "                    duplicateFilesMap[md5Hash] = [uniqueFilesMap[md5Hash], file]\n",
    "            else:\n",
    "                uniqueFilesMap[md5Hash] = file\n",
    "    return duplicateFilesMap\n",
    "\n",
    "\n",
    "## Moves the inputFile to the duplicatesFolder\n",
    "def moveFile(inputFile, duplicatesFolder):\n",
    "    try:\n",
    "        ## To join two absolute paths, we need to split the second path into its components and path\n",
    "        ## that as an expanded list\n",
    "        destFolder = path.join(duplicatesFolder, *path.normpath(path.dirname(inputFile)).split(sep))\n",
    "\n",
    "        makeDirLock.acquire()\n",
    "        ## Create the dest folder if necessary\n",
    "        if (not path.exists(destFolder)):\n",
    "            makedirs(destFolder)\n",
    "        makeDirLock.release()\n",
    "\n",
    "        shutil.move(inputFile, path.join(destFolder, path.basename(inputFile)))\n",
    "        #print (inputFile)\n",
    "    except:\n",
    "        logging.exception(\"\")\n",
    "        raise\n",
    "        \n",
    "## moves all the duplicates from the duplicateFilesMap to duplicatesDestRoot folder and preserves\n",
    "## the original directory structure\n",
    "def moveTheDuplicates(duplicateFilesMap, duplicatesDestRoot):\n",
    "    # We can use a with statement to ensure threads are cleaned up promptly\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        for duplicateFiles in duplicateFilesMap.values():\n",
    "            if (len(duplicateFiles) > 1):\n",
    "                future_to_file = {executor.submit(moveFile, duplicateFile, duplicatesDestRoot): duplicateFile for duplicateFile in duplicateFiles[1:]}\n",
    "                for future in concurrent.futures.as_completed(future_to_file):\n",
    "                    file = future_to_file[future]\n",
    "            else:\n",
    "                raise Exception(\"how did you get a single file entry in duplicate files list?\")\n",
    "             \n",
    "            \n",
    "## Saves the duplicate and unique files list in a json file\n",
    "def saveFileList(uniqueFilesMap, uniqueFilesMapPath, duplicateFilesMap, duplicateFilesMapPath):\n",
    "    duplicateFilesMapJson = json.JSONEncoder(sort_keys=True, indent=0).encode(duplicateFilesMap)\n",
    "    with open(duplicateFilesMapPath, 'w') as jsonFile:\n",
    "        jsonFile.write(duplicateFilesMapJson)\n",
    "\n",
    "   \n",
    "    uniqueFilesMapJson = json.JSONEncoder(sort_keys=True, indent=0).encode(uniqueFilesMap)\n",
    "    with open(uniqueFilesMapPath, 'w') as jsonFile:\n",
    "        jsonFile.write(uniqueFilesMapJson)\n",
    "\n",
    "def main():\n",
    "    inputPaths=['/src/data/test1/', '/src/data/test2/']\n",
    "    duplicatesPath='/src/data/duplicates'\n",
    "    uniqueFilesMapPath='/src/data/uniqueFiles.json'\n",
    "    duplicateFilesMapPath='/src/data/duplicateFiles.json'\n",
    "    \n",
    "    uniqueFilesMap=loadUniqueFilesMap(uniqueFilesMapPath)\n",
    "    inputFiles = buildInputFilesList(inputPaths, uniqueFilesMap)\n",
    "    duplicateFilesMap = checkForDuplicates(inputFiles, uniqueFilesMap)\n",
    "    moveTheDuplicates(duplicateFilesMap, duplicatesPath)\n",
    "    saveFileList(uniqueFilesMap, uniqueFilesMapPath, duplicateFilesMap, duplicateFilesMapPath)\n",
    "\n",
    "\n",
    "%lprun -f loadUniqueFilesMap main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
