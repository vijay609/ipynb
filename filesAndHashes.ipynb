{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot,image\n",
    "\n",
    "shanthiPic = '/src/shanthiPictures/101ND750/DSC_0181.JPG'\n",
    "img = image.imread(shanthiPic)\n",
    "imgplot=pyplot.imshow(img)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def calculateMD5Hash(file):\n",
    "    fh = open(file, \"rb\")\n",
    "    return hashlib.md5(fh.read()).hexdigest()\n",
    "    \n",
    "\n",
    "shanthiPic = '/src/data/testPythonDuplicateFinder/2015-09/IMG_1313.JPG'\n",
    "print(calculateMD5Hash(shanthiPic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### List Extention ###\n",
    "from os import walk as oswalk\n",
    "from os.path import join as osjoin\n",
    "from time import time\n",
    "inputPath = '/src/data/testPythonDuplicateFinder'\n",
    "startTime = time()\n",
    "imageHashes = []\n",
    "images =[]\n",
    "for root, dirs, files in oswalk(inputPath):\n",
    "    images = [osjoin(root, f) for f in files]\n",
    "    imageHashes.extend([(image, calculateMD5Hash(image)) for image in images])\n",
    "\n",
    "print('Took {}s for {} images'.format(time() - startTime, len(imageHashes)))\n",
    "print ('images {}'.format(len(images)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from os import walk as oswalk\n",
    "from os.path import join as osjoin\n",
    "from time import time\n",
    "import hashlib\n",
    "def calculateMD5Hash(file):\n",
    "    fh = open(file, \"rb\")\n",
    "    return hashlib.md5(fh.read()).hexdigest()\n",
    "    \n",
    "inputPaths = ['/src/data/testPythonDuplicateFinder']\n",
    "duplicatesPath = '/src/data/duplicates'\n",
    "inputImages = []\n",
    "for inputPath in inputPaths:\n",
    "    for root, dirs, files in oswalk(inputPath):\n",
    "        inputImages.extend(img for img in [osjoin(root, f) for f in files] if img not in inputImages)\n",
    "\n",
    "\n",
    "startTime = time()\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_file = {executor.submit(calculateMD5Hash, inputFile): inputFile for inputFile in inputImages}\n",
    "    imageHashMap = {}\n",
    "    for future in concurrent.futures.as_completed(future_to_file):\n",
    "        file = future_to_file[future]\n",
    "        try:\n",
    "            md5Hash = future.result()\n",
    "            \n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (file, exc))\n",
    "        else:\n",
    "            if md5Hash in imageHashMap: \n",
    "                imageHashMap[md5Hash].extend([file])\n",
    "            else:\n",
    "                imageHashMap[md5Hash] = [file]\n",
    "\n",
    "print (imageHashMap)\n",
    "print('Took {}s for  {} images'.format(time() - startTime, len(inputImages)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "imageHashMapJson = json.JSONEncoder(sort_keys=True, indent=0).encode(imageHashMap)\n",
    "print (imageHashMapJson)\n",
    "with open('/src/data/fileHashes.json', 'w') as jsonFile:\n",
    "    jsonFile.write(imageHashMapJson)\n",
    "\n",
    "#json.dump(imageHashMap, '/src/data/fileHashes.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two ways to log an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:hello\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-77-8eb620650385>\", line 9, in <module>\n",
      "    print (os.path.normpath('/data'))\n",
      "NameError: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-77-8eb620650385>\", line 3, in <module>\n",
      "    print (os.path.normpath('/data'))\n",
      "NameError: name 'os' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Two ways to log exception\n",
    "import traceback\n",
    "try:\n",
    "    print (os.path.normpath('/data'))\n",
    "except :\n",
    "    print (traceback.format_exc())\n",
    "    \n",
    "import logging\n",
    "try:\n",
    "    print (os.path.normpath('/data'))\n",
    "except :\n",
    "    logging.exception(\"hello\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## Data Setup\n",
    "rm -rf /src/data/test1/ /src/data/test2/ /src/data/duplicates\n",
    "cp -r '/src/data/test1 (copy)/' '/src/data/test1/'\n",
    "cp -r '/src/data/test2 (copy)/' '/src/data/test2/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346\n",
      "checkForDuplicates took 2.2411575317382812s\n",
      "inputImages length 346\n",
      "knownImagesMap length 423\n",
      "\n",
      "moveTheDuplicates took 0.10678672790527344s\n"
     ]
    }
   ],
   "source": [
    "## Start by loading the master list\n",
    "## Prepare the list of unique file paths\n",
    "## Find duplicates using the hash\n",
    "## move duplicates to duplicates folder\n",
    "## the run should output [hash, originalFilePath, [duplciate 1 path, /duplicate 2 path, duplicate 3,....]]\n",
    "## save this master list along with the hash [hash, original file path]\n",
    "import json\n",
    "from os import walk, path, sep, makedirs\n",
    "\n",
    "import concurrent.futures\n",
    "import hashlib\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "import threading\n",
    "\n",
    "def buildKnownFilesList(masterList):\n",
    "    if (path.exists(masterList)):\n",
    "        return json.load(open(masterList))\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def buildInputFilesList(inputPaths, knownImagesMap):\n",
    "    inputImages = []\n",
    "    for inputPath in inputPaths:\n",
    "        for root, dirs, files in walk(inputPath):\n",
    "            inputImages.extend(img for img in [path.join(root, f) for f in files] if img not in inputImages if img not in knownImagesMap.values())\n",
    "    return inputImages\n",
    "\n",
    "def calculateMD5Hash(file):\n",
    "    fh = open(file, \"rb\")\n",
    "    return hashlib.md5(fh.read()).hexdigest()\n",
    "\n",
    "def checkForDuplicates(inputImages, knownImagesMap):\n",
    "    duplicateImagesMap = {}\n",
    "    # We can use a with statement to ensure threads are cleaned up promptly\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        future_to_file = {executor.submit(calculateMD5Hash, inputFile): inputFile for inputFile in inputImages}\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file = future_to_file[future]\n",
    "            try:\n",
    "                md5Hash = future.result()\n",
    "\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (file, exc))\n",
    "            else:\n",
    "                if md5Hash in knownImagesMap:\n",
    "                    if md5Hash in duplicateImagesMap:\n",
    "                        duplicateImagesMap[md5Hash].append(file)\n",
    "                    else:\n",
    "                        duplicateImagesMap[md5Hash] = [knownImagesMap[md5Hash], file]\n",
    "                else:\n",
    "                    knownImagesMap[md5Hash] = file\n",
    "    return duplicateImagesMap\n",
    "\n",
    "def moveFile(inputFile, duplicatesFolder):\n",
    "    try:\n",
    "        ## To join two absolute paths, we need to split the second path into its components and path\n",
    "        ## that as an expanded list\n",
    "        destFolder = path.join(duplicatesFolder, *path.normpath(path.dirname(inputFile)).split(sep))\n",
    "\n",
    "        lock = threading.RLock()\n",
    "        lock.acquire()\n",
    "        ## Create the dest folder if necessary\n",
    "        if (not path.exists(destFolder)):\n",
    "            makedirs(destFolder)\n",
    "        lock.release()\n",
    "\n",
    "        shutil.move(inputFile, path.join(destFolder, path.basename(inputFile)))\n",
    "        #print (inputFile)\n",
    "    except:\n",
    "        logging.exception(\"\")\n",
    "        raise\n",
    "        \n",
    "    \n",
    "def moveTheDuplicates(duplicateImagesMap, duplicatesDestRoot):\n",
    "    # We can use a with statement to ensure threads are cleaned up promptly\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        for duplicateFiles in duplicateImagesMap.values():\n",
    "            if (len(duplicateFiles) > 1):\n",
    "                future_to_file = {executor.submit(moveFile, duplicateFile, duplicatesDestRoot): duplicateFile for duplicateFile in duplicateFiles[1:]}\n",
    "                for future in concurrent.futures.as_completed(future_to_file):\n",
    "                    file = future_to_file[future]\n",
    "            else:\n",
    "                raise Exception(\"how did you get a single file entry in duplicate files list?\")\n",
    "             \n",
    "            \n",
    "\n",
    "def saveFileList(knownImagesMap, uniqueFilesMapPath, duplicateImagesMap, duplicateFilesMapPath):\n",
    "    duplicateFilesMapJson = json.JSONEncoder(sort_keys=True, indent=0).encode(duplicateImagesMap)\n",
    "    with open(duplicateFilesMapPath, 'w') as jsonFile:\n",
    "        jsonFile.write(duplicateFilesMapJson)\n",
    "\n",
    "   \n",
    "    uniqueFilesMapJson = json.JSONEncoder(sort_keys=True, indent=0).encode(knownImagesMap)\n",
    "    with open(uniqueFilesMapPath, 'w') as jsonFile:\n",
    "        jsonFile.write(uniqueFilesMapJson)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    inputPaths=['/src/data/test1/', '/src/data/test2/']\n",
    "    duplicatesPath='/src/data/duplicates'\n",
    "    uniqueFilesMapPath='/src/data/uniqueFiles.json'\n",
    "    duplicateFilesMapPath='/src/data/duplicateFiles.json'\n",
    "    startTime = time.time()\n",
    "\n",
    "    knownImagesMap=buildKnownFilesList(uniqueFilesMapPath)\n",
    "\n",
    "    inputImages = buildInputFilesList(inputPaths, knownImagesMap)\n",
    "    print (len(inputImages))\n",
    "    duplicateImagesMap = checkForDuplicates(inputImages, knownImagesMap)\n",
    "    print('checkForDuplicates took {}s'.format(time.time() - startTime))\n",
    "    print ('inputImages length {}'.format(len(inputImages)))\n",
    "    print ('knownImagesMap length {}'.format(len(knownImagesMap)))\n",
    "    \n",
    "    startTime = time.time()\n",
    "    print()\n",
    "    moveTheDuplicates(duplicateImagesMap, duplicatesPath)\n",
    "    print('moveTheDuplicates took {}s'.format(time.time() - startTime))\n",
    "    saveFileList(knownImagesMap, uniqueFilesMapPath, duplicateImagesMap, duplicateFilesMapPath)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
