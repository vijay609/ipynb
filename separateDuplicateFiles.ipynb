{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot,image\n",
    "\n",
    "shanthiPic = '/src/shanthiPictures/101ND750/DSC_0181.JPG'\n",
    "img = image.imread(shanthiPic)\n",
    "imgplot=pyplot.imshow(img)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/src/data/testPythonDuplicateFinder/2015-09/IMG_1313.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b48794febf69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mshanthiPic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/src/data/testPythonDuplicateFinder/2015-09/IMG_1313.JPG'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculateMD5Hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshanthiPic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-b48794febf69>\u001b[0m in \u001b[0;36mcalculateMD5Hash\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculateMD5Hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/src/data/testPythonDuplicateFinder/2015-09/IMG_1313.JPG'"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "def calculateMD5Hash(file):\n",
    "    fh = open(file, \"rb\")\n",
    "    return hashlib.md5(fh.read()).hexdigest()\n",
    "    \n",
    "\n",
    "shanthiPic = '/src/data/testPythonDuplicateFinder/2015-09/IMG_1313.JPG'\n",
    "print(calculateMD5Hash(shanthiPic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### List Extention ###\n",
    "from os import walk as oswalk\n",
    "from os.path import join as osjoin\n",
    "from time import time\n",
    "inputPath = '/src/data/testPythonDuplicateFinder'\n",
    "startTime = time()\n",
    "imageHashes = []\n",
    "images =[]\n",
    "for root, dirs, files in oswalk(inputPath):\n",
    "    images = [osjoin(root, f) for f in files]\n",
    "    imageHashes.extend([(image, calculateMD5Hash(image)) for image in images])\n",
    "\n",
    "print('Took {}s for {} images'.format(time() - startTime, len(imageHashes)))\n",
    "print ('images {}'.format(len(images)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from os import walk as oswalk\n",
    "from os.path import join as osjoin\n",
    "from time import time\n",
    "import hashlib\n",
    "def calculateMD5Hash(file):\n",
    "    fh = open(file, \"rb\")\n",
    "    return hashlib.md5(fh.read()).hexdigest()\n",
    "    \n",
    "inputPaths = ['/src/data/testPythonDuplicateFinder']\n",
    "duplicatesPath = '/src/data/duplicates'\n",
    "inputImages = []\n",
    "for inputPath in inputPaths:\n",
    "    for root, dirs, files in oswalk(inputPath):\n",
    "        inputImages.extend(img for img in [osjoin(root, f) for f in files] if img not in inputImages)\n",
    "\n",
    "\n",
    "startTime = time()\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_file = {executor.submit(calculateMD5Hash, inputFile): inputFile for inputFile in inputImages}\n",
    "    imageHashMap = {}\n",
    "    for future in concurrent.futures.as_completed(future_to_file):\n",
    "        file = future_to_file[future]\n",
    "        try:\n",
    "            md5Hash = future.result()\n",
    "            \n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (file, exc))\n",
    "        else:\n",
    "            if md5Hash in imageHashMap: \n",
    "                imageHashMap[md5Hash].extend([file])\n",
    "            else:\n",
    "                imageHashMap[md5Hash] = [file]\n",
    "\n",
    "print (imageHashMap)\n",
    "print('Took {}s for  {} images'.format(time() - startTime, len(inputImages)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "imageHashMapJson = json.JSONEncoder(sort_keys=True, indent=0).encode(imageHashMap)\n",
    "print (imageHashMapJson)\n",
    "with open('/src/data/fileHashes.json', 'w') as jsonFile:\n",
    "    jsonFile.write(imageHashMapJson)\n",
    "\n",
    "#json.dump(imageHashMap, '/src/data/fileHashes.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two ways to log an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:hello\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-77-8eb620650385>\", line 9, in <module>\n",
      "    print (os.path.normpath('/data'))\n",
      "NameError: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-77-8eb620650385>\", line 3, in <module>\n",
      "    print (os.path.normpath('/data'))\n",
      "NameError: name 'os' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Two ways to log exception\n",
    "import traceback\n",
    "try:\n",
    "    print (os.path.normpath('/data'))\n",
    "except :\n",
    "    print (traceback.format_exc())\n",
    "    \n",
    "import logging\n",
    "try:\n",
    "    print (os.path.normpath('/data'))\n",
    "except :\n",
    "    logging.exception(\"hello\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "## Data Setup\n",
    "rm -rf /src/data/test1/ /src/data/test2/ /src/data/duplicates /src/data/*.json\n",
    "cp -r '/src/data/test1 (copy)/' '/src/data/test1/'\n",
    "cp -r '/src/data/test2 (copy)/' '/src/data/test2/'\n",
    "cp -r '/src/data/test2 (copy)/' '/src/data/test2/2015-10/'\n",
    "cp -r '/src/data/test1 (copy)/' '/src/data/test1/2015-09/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "#%%time\n",
    "## Start by loading the uniqueFiles list\n",
    "## Prepare the list of unique file paths\n",
    "## Find duplicates using the hash\n",
    "## move duplicates to duplicates folder\n",
    "## the run should output [hash, originalFilePath, [duplciate 1 path, /duplicate 2 path, duplicate 3,....]]\n",
    "## save this uniqueFiles list along with the hash [hash, original file path]\n",
    "import json\n",
    "from os import walk, path, sep, makedirs\n",
    "\n",
    "import concurrent.futures\n",
    "import hashlib\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "import threading\n",
    "makeDirLock = threading.Lock()\n",
    "\n",
    "## loads the file paths and hashes that are previously known\n",
    "## also ensures that all those files are actually present on the disk\n",
    "def loadUniqueFilesMap(uniqueFilesMapFile):\n",
    "    uniqueFilesMap = json.load(open(uniqueFilesMapFile)) if path.exists(uniqueFilesMapFile) else {}\n",
    "     # We can use a with statement to ensure threads are cleaned up promptly\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        future_to_file = {executor.submit(path.exists, inputFile): inputFile for inputFile in uniqueFilesMap.values()}\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            if (not future.result()):\n",
    "                raise OSError(\"File not found : {}\".format(future_to_file[future]))\n",
    "\n",
    "    return uniqueFilesMap\n",
    "\n",
    "## creates and returns a list of all the file paths from the inputPaths that are not\n",
    "## known(not present in uniqueFilesMap)\n",
    "def buildInputFilesList(inputPaths, uniqueFilesMap):\n",
    "    inputFiles = []\n",
    "    for inputPath in inputPaths:\n",
    "        for root, dirs, files in walk(inputPath):\n",
    "            inputFiles.extend(img for img in [path.join(root, f) for f in files] if img not in inputFiles if img not in uniqueFilesMap.values())\n",
    "    return inputFiles\n",
    "\n",
    "def calculateMD5Hash(file):\n",
    "    fh = open(file, \"rb\")\n",
    "    return hashlib.md5(fh.read()).hexdigest()\n",
    "\n",
    "## find all the inputFiles that are already present in uniqueFilesMap and add them to the duplicateFilesMap\n",
    "## all the new files frim teh inputFiles get added to uniqueFilesMap\n",
    "def checkForDuplicates(inputFiles, uniqueFilesMap):\n",
    "    duplicateFilesMap = {}\n",
    "    # We can use a with statement to ensure threads are cleaned up promptly\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        future_to_file = {executor.submit(calculateMD5Hash, inputFile): inputFile for inputFile in inputFiles}\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file = future_to_file[future]\n",
    "            md5Hash = future.result()\n",
    "            if md5Hash in uniqueFilesMap:\n",
    "                if md5Hash in duplicateFilesMap:\n",
    "                    duplicateFilesMap[md5Hash].append(file)\n",
    "                else:\n",
    "                    duplicateFilesMap[md5Hash] = [uniqueFilesMap[md5Hash], file]\n",
    "            else:\n",
    "                uniqueFilesMap[md5Hash] = file\n",
    "    return duplicateFilesMap\n",
    "\n",
    "\n",
    "## Moves the inputFile to the duplicatesFolder\n",
    "def moveFile(inputFile, duplicatesFolder):\n",
    "    try:\n",
    "        ## To join two absolute paths, we need to split the second path into its components and path\n",
    "        ## that as an expanded list\n",
    "        destFolder = path.join(duplicatesFolder, *path.normpath(path.dirname(inputFile)).split(sep))\n",
    "\n",
    "        makeDirLock.acquire()\n",
    "        ## Create the dest folder if necessary\n",
    "        if (not path.exists(destFolder)):\n",
    "            makedirs(destFolder)\n",
    "        makeDirLock.release()\n",
    "\n",
    "        shutil.move(inputFile, path.join(destFolder, path.basename(inputFile)))\n",
    "        #print (inputFile)\n",
    "    except:\n",
    "        logging.exception(\"\")\n",
    "        raise\n",
    "        \n",
    "## moves all the duplicates from the duplicateFilesMap to duplicatesDestRoot folder and preserves\n",
    "## the original directory structure\n",
    "def moveTheDuplicates(duplicateFilesMap, duplicatesDestRoot):\n",
    "    # We can use a with statement to ensure threads are cleaned up promptly\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        future_to_file = {}\n",
    "        for duplicateFiles in duplicateFilesMap.values():\n",
    "            if (len(duplicateFiles) > 1):\n",
    "                ## This will merge the new dict returned by executer.submit with future_to_file and save the result\n",
    "                ## in future_to_file\n",
    "                future_to_file = {**future_to_file, **{executor.submit(moveFile, duplicateFile, duplicatesDestRoot): duplicateFile for duplicateFile in duplicateFiles[1:]}}\n",
    "            else:\n",
    "                raise Exception(\"how did you get a single file entry in duplicate files list?\")\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            pass\n",
    "            \n",
    "## Saves the duplicate and unique files list in a json file\n",
    "def saveFileList(uniqueFilesMap, uniqueFilesMapPath, duplicateFilesMap, duplicateFilesMapPath):\n",
    "    duplicateFilesMapJson = json.JSONEncoder(sort_keys=True, indent=0).encode(duplicateFilesMap)\n",
    "    with open(duplicateFilesMapPath, 'w') as jsonFile:\n",
    "        jsonFile.write(duplicateFilesMapJson)\n",
    "\n",
    "   \n",
    "    uniqueFilesMapJson = json.JSONEncoder(sort_keys=True, indent=0).encode(uniqueFilesMap)\n",
    "    with open(uniqueFilesMapPath, 'w') as jsonFile:\n",
    "        jsonFile.write(uniqueFilesMapJson)\n",
    "\n",
    "def main():\n",
    "    inputPaths=['/src/data/test1/', '/src/data/test2/']\n",
    "    duplicatesPath='/src/data/duplicates'\n",
    "    uniqueFilesMapPath='/src/data/uniqueFiles.json'\n",
    "    duplicateFilesMapPath='/src/data/duplicateFiles.json'\n",
    "    \n",
    "    uniqueFilesMap=loadUniqueFilesMap(uniqueFilesMapPath)\n",
    "    inputFiles = buildInputFilesList(inputPaths, uniqueFilesMap)\n",
    "    duplicateFilesMap = checkForDuplicates(inputFiles, uniqueFilesMap)\n",
    "    moveTheDuplicates(duplicateFilesMap, duplicatesPath)\n",
    "    saveFileList(uniqueFilesMap, uniqueFilesMapPath, duplicateFilesMap, duplicateFilesMapPath)\n",
    "\n",
    "\n",
    "#%lprun -f loadUniqueFilesMap main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure duplicates are actually duplicates by checking the size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115 future_to_file size\n",
      "17605 future_to_file size\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "def haveSameSize(file1, file2):\n",
    "    return os.stat(file1).st_size == os.stat(file2).st_size\n",
    "\n",
    "## loads the file paths and hashes that are previously known\n",
    "## also ensures that all those files are actually present on the disk\n",
    "def loadDuplicateFilesMap(duplicateFilesMapFile):\n",
    "    if os.path.exists(duplicateFilesMapFile):\n",
    "        duplicateFilesMap = json.load(open(duplicateFilesMapFile))\n",
    "\n",
    "        # We can use a with statement to ensure threads are cleaned up promptly\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future_to_file = {}\n",
    "            # Start the load operations and mark each future with its URL\n",
    "            for duplicateFiles in duplicateFilesMap.values():\n",
    "                if (len(duplicateFiles) < 1):\n",
    "                    raise Exception(\"how did you get a single file entry in duplicate files list?\")\n",
    "                future_to_file = {**future_to_file, **{executor.submit(haveSameSize, duplicateFiles[0], os.path.join(duplicatesBasePath, *os.path.normpath(duplicateFile).split(os.sep))): duplicateFile for duplicateFile in duplicateFiles[1:]}}\n",
    "            print ('{} future_to_file size'.format(len(future_to_file)))    \n",
    "            for future in concurrent.futures.as_completed(future_to_file):\n",
    "                if not future.result():\n",
    "                    print ('{} is a false duplicate'.format(future_to_file[future]))\n",
    "\n",
    "duplicatesBasePath = '/src/data/duplicates/'\n",
    "loadDuplicateFilesMap('/src/data/duplicateFiles.json')\n",
    "\n",
    "duplicatesBasePath = '/media/vijay/Data/Pictures/duplicates/'\n",
    "loadDuplicateFilesMap('/media/vijay/Data/Pictures/logs/duplicateFiles.json')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UnitTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageColor\n",
    "import os.path\n",
    "\n",
    "def createTestData(basepath):\n",
    "    imgArr = np.zeros(shape=[100,100,3], dtype=np.uint8)\n",
    "    if not os.path.exists(basepath):\n",
    "        os.makedirs(basepath)\n",
    "    colorsAdded = set()\n",
    "    for col in ImageColor.colormap.keys():\n",
    "        colorString = ImageColor.colormap[col]\n",
    "        if colorString not in colorsAdded:\n",
    "            imgArr[:,:,:] = ImageColor.getrgb(colorString)\n",
    "            Image.fromarray(imgArr, mode='RGB').save(os.path.join(basepath,'img{}.png'.format(col)))\n",
    "            colorsAdded.add(colorString)\n",
    "createTestData('/data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestStringMethods(unittest.TestCase):\n",
    "\n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "    def test_split(self):\n",
    "        s = 'hello world'\n",
    "        self.assertEqual(s.split(), ['hello', 'world'])\n",
    "        # check that s.split fails when the separator is not a string\n",
    "        with self.assertRaises(TypeError):\n",
    "            s.split(2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "#%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.073s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import os.path\n",
    "import shutil\n",
    "\n",
    "class SimpleTest(unittest.TestCase):\n",
    "    basePath = '/data/test'\n",
    "    def setUp(self):\n",
    "        if os.path.exists(SimpleTest.basePath):\n",
    "            shutil.rmtree(SimpleTest.basePath)\n",
    "        createTestData(SimpleTest.basePath)\n",
    "        \n",
    "    def test_uniqueFiles(self):\n",
    "        os.makedirs(os.path.join(SimpleTest.basePath,'1'))\n",
    "        os.makedirs(os.path.join(SimpleTest.basePath,'2'))\n",
    "        os.makedirs(os.path.join(SimpleTest.basePath,'3'))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "#%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
